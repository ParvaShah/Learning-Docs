# Kafka Leader Failure: Complete Analysis
## What Happens When a Leader Broker Fails

---

## Table of Contents
1. [The Baseline Scenario](#the-baseline-scenario)
2. [Understanding Key Offsets](#understanding-key-offsets)
3. [Leader Failure Timeline](#leader-failure-timeline)
4. [Impact of Different Configurations](#impact-of-different-configurations)
5. [The Critical Question: Unreplicated Messages](#the-critical-question-unreplicated-messages)
6. [Producer Behavior During Failure](#producer-behavior-during-failure)
7. [Consumer Behavior During Failure](#consumer-behavior-during-failure)
8. [Data Loss Scenarios](#data-loss-scenarios)
9. [Configuration Best Practices](#configuration-best-practices)

---

## The Baseline Scenario

Let's establish a concrete example to work with:

```
SETUP:
Topic: orders
Partition: 0
Replication Factor: 3
Brokers: 1, 2, 3

BEFORE FAILURE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PARTITION: orders-0                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              BROKER 1 (LEADER)                           â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Offsets: 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ 1005 â”‚  â”‚
â”‚  â”‚           [============ MESSAGES =============]          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  LEO (Log End Offset): 1005                              â”‚  â”‚
â”‚  â”‚  HWM (High Water Mark): 1000                             â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Messages 0-999:   âœ“ Replicated to all followers        â”‚  â”‚
â”‚  â”‚  Messages 1000-1004: âš ï¸  Only on leader (not replicated) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              BROKER 2 (FOLLOWER)                         â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Offsets: 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ 1000  â”‚  â”‚
â”‚  â”‚           [============ MESSAGES =============]          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  LEO: 1000                                               â”‚  â”‚
â”‚  â”‚  Currently fetching from leader...                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              BROKER 3 (FOLLOWER)                         â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Offsets: 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ 1000  â”‚  â”‚
â”‚  â”‚           [============ MESSAGES =============]          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  LEO: 1000                                               â”‚  â”‚
â”‚  â”‚  Currently fetching from leader...                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  ISR (In-Sync Replicas): [1, 2, 3]                             â”‚
â”‚                                                                 â”‚
â”‚  KEY POINT:                                                     â”‚
â”‚  - Messages 1000-1004 exist ONLY on Broker 1 (leader)          â”‚
â”‚  - These messages are NOT visible to consumers (below HWM)      â”‚
â”‚  - Followers are about to fetch these messages                  â”‚
â”‚                                                                 â”‚
â”‚         âŒ BROKER 1 CRASHES NOW âŒ                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Understanding Key Offsets

Before we dive into failures, let's be crystal clear on these terms:

### LEO (Log End Offset)

```
Definition: The offset of the NEXT message to be written
           (or the offset after the last message)

Leader LEO: 1005
  â†’ Last message is at offset 1004
  â†’ Next message will be at offset 1005

Follower LEO: 1000
  â†’ Last message is at offset 999
  â†’ Next message will be at offset 1000
  â†’ This follower is 5 messages behind leader
```

### HWM (High Water Mark)

```
Definition: The smallest LEO among all ISR members
           (The last offset that ALL ISR replicas have)

Calculation:
  Leader LEO: 1005
  Follower 1 LEO: 1000
  Follower 2 LEO: 1000

  HWM = min(1005, 1000, 1000) = 1000

Importance:
  - Only messages BELOW HWM are visible to consumers
  - Guarantees consumers only see replicated data
  - Ensures consistency even if leader fails

Visual:
  Offset:   0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1000 â”€â”€â”€ 1005
            â”‚                             â”‚      â”‚
            â”‚â—„â”€â”€ Consumers can read â”€â”€â”€â”€â”€â–¶â”‚      â”‚
            â”‚                             â”‚      â”‚
            LSO                          HWM    LEO
         (start)                     (replicated)(end)
```

### Why This Matters

```
Messages 0-999:
  âœ“ Below HWM
  âœ“ Replicated to all ISR members
  âœ“ Visible to consumers
  âœ“ Safe from data loss (exist on multiple brokers)

Messages 1000-1004:
  âœ— Above HWM
  âœ— Only on leader
  âœ— NOT visible to consumers (yet)
  âš ï¸  At risk if leader fails NOW
```

---

## Leader Failure Timeline

### Step-by-Step: What Actually Happens

```
TIME T=0: BROKER 1 CRASHES
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Broker 1 (Leader): âœ—âœ—âœ— CRASH âœ—âœ—âœ—                              â”‚
â”‚                                                                 â”‚
â”‚  - Hardware failure / OOM / Network partition / Process kill    â”‚
â”‚  - All messages 1000-1004 exist ONLY on this dead broker        â”‚
â”‚  - Producer trying to send gets NetworkException                â”‚
â”‚  - Consumers trying to fetch get NetworkException               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME T=0 to T=6s: NOBODY KNOWS YET
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Controller (Broker 4, let's say):                              â”‚
â”‚    - Waiting for heartbeat from Broker 1                        â”‚
â”‚    - Heartbeat expected every 3 seconds                         â”‚
â”‚    - Timeout configured: 6 seconds (default)                    â”‚
â”‚    - Still within timeout window...                             â”‚
â”‚                                                                 â”‚
â”‚  Broker 2 (Follower):                                           â”‚
â”‚    - Tries to fetch from Broker 1                               â”‚
â”‚    - Gets connection error                                      â”‚
â”‚    - Retries...                                                 â”‚
â”‚                                                                 â”‚
â”‚  Broker 3 (Follower):                                           â”‚
â”‚    - Tries to fetch from Broker 1                               â”‚
â”‚    - Gets connection error                                      â”‚
â”‚    - Retries...                                                 â”‚
â”‚                                                                 â”‚
â”‚  Producers:                                                     â”‚
â”‚    - Trying to send to Broker 1                                 â”‚
â”‚    - Getting connection errors                                  â”‚
â”‚    - Retrying (if configured)                                   â”‚
â”‚    - Requests queuing up in buffer                              â”‚
â”‚                                                                 â”‚
â”‚  Consumers:                                                     â”‚
â”‚    - Trying to fetch from Broker 1                              â”‚
â”‚    - Getting connection errors                                  â”‚
â”‚    - Retrying...                                                â”‚
â”‚                                                                 â”‚
â”‚  STATUS: PARTITION UNAVAILABLE (nobody can read or write)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME T=6s: CONTROLLER DETECTS FAILURE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Controller:                                                    â”‚
â”‚    1. Broker 1 heartbeat timeout exceeded (6 seconds)           â”‚
â”‚    2. Mark Broker 1 as DOWN                                     â”‚
â”‚    3. Identify affected partitions where Broker 1 is leader:    â”‚
â”‚       - orders-0                                                â”‚
â”‚       - payments-1                                              â”‚
â”‚       - customers-2                                             â”‚
â”‚       - (etc... could be hundreds of partitions)                â”‚
â”‚    4. Start leader election process for each partition          â”‚
â”‚                                                                 â”‚
â”‚  âš™ï¸  LEADER ELECTION BEGINS âš™ï¸                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME T=6s to T=6.1s: LEADER ELECTION FOR orders-0
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Controller's election process:                                 â”‚
â”‚                                                                 â”‚
â”‚  Step 1: Get current state                                      â”‚
â”‚    Current ISR: [1, 2, 3]                                       â”‚
â”‚    Current Leader: 1 (DEAD)                                     â”‚
â”‚                                                                 â”‚
â”‚  Step 2: Remove dead broker from ISR                            â”‚
â”‚    New ISR: [2, 3]                                              â”‚
â”‚                                                                 â”‚
â”‚  Step 3: Select new leader                                      â”‚
â”‚    Algorithm: First replica in ISR that is alive                â”‚
â”‚    Preference order: [2, 3]                                     â”‚
â”‚    New Leader: 2 (Broker 2) âœ“                                  â”‚
â”‚                                                                 â”‚
â”‚  Step 4: Increment leader epoch                                 â”‚
â”‚    Old epoch: 5                                                 â”‚
â”‚    New epoch: 6                                                 â”‚
â”‚    (Used to reject stale requests)                              â”‚
â”‚                                                                 â”‚
â”‚  Step 5: Update metadata                                        â”‚
â”‚    Leader: 2                                                    â”‚
â”‚    ISR: [2, 3]                                                  â”‚
â”‚    Epoch: 6                                                     â”‚
â”‚    Partition state: ONLINE                                      â”‚
â”‚                                                                 â”‚
â”‚  Step 6: Send LeaderAndIsrRequest                               â”‚
â”‚    To Broker 2: "You are now leader for orders-0"              â”‚
â”‚    To Broker 3: "New leader for orders-0 is Broker 2"          â”‚
â”‚                                                                 â”‚
â”‚  Step 7: Send UpdateMetadataRequest                             â”‚
â”‚    To ALL brokers: "orders-0 leader is now Broker 2"           â”‚
â”‚    (So producers/consumers can redirect)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME T=6.1s: NEW LEADER TAKES OVER
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PARTITION: orders-0                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              BROKER 2 (NEW LEADER) â˜…                     â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Offsets: 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ 1000  â”‚  â”‚
â”‚  â”‚           [============ MESSAGES =============]          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  LEO: 1000                                               â”‚  â”‚
â”‚  â”‚  HWM: 1000 (same as LEO now, since fully caught up)     â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Actions taken:                                          â”‚  â”‚
â”‚  â”‚  1. Accept produce requests                              â”‚  â”‚
â”‚  â”‚  2. Accept fetch requests                                â”‚  â”‚
â”‚  â”‚  3. Start tracking follower progress                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              BROKER 3 (FOLLOWER)                         â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  Offsets: 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ 1000  â”‚  â”‚
â”‚  â”‚           [============ MESSAGES =============]          â”‚  â”‚
â”‚  â”‚                                                          â”‚  â”‚
â”‚  â”‚  LEO: 1000                                               â”‚  â”‚
â”‚  â”‚  Now fetching from Broker 2 (new leader)                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  ISR: [2, 3]                                                    â”‚
â”‚  Leader: Broker 2                                               â”‚
â”‚  Leader Epoch: 6                                                â”‚
â”‚                                                                 â”‚
â”‚  âš ï¸  CRITICAL OBSERVATION:                                      â”‚
â”‚  Messages 1000-1004 from old leader are LOST                    â”‚
â”‚  They never made it to any follower                             â”‚
â”‚  They were not visible to consumers (above HWM)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TIME T=6.1s+: CLIENTS RECOVER
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Producers:                                                     â”‚
â”‚    1. Get metadata refresh showing new leader                   â”‚
â”‚    2. Reconnect to Broker 2                                     â”‚
â”‚    3. Retry buffered messages (if retries configured)           â”‚
â”‚    4. Resume sending to Broker 2                                â”‚
â”‚                                                                 â”‚
â”‚  Consumers:                                                     â”‚
â”‚    1. Get metadata refresh showing new leader                   â”‚
â”‚    2. Reconnect to Broker 2                                     â”‚
â”‚    3. Continue fetching from last committed offset              â”‚
â”‚    4. No messages lost from consumer perspective                â”‚
â”‚       (They never saw 1000-1004 anyway)                         â”‚
â”‚                                                                 â”‚
â”‚  TOTAL DOWNTIME: ~100ms (from T=6s to T=6.1s)                   â”‚
â”‚    - Time for election and metadata propagation                 â”‚
â”‚    - Actual detection took 6 seconds (heartbeat timeout)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Visual Timeline

```
T=0s      Leader crashes, messages 1000-1004 lost
          âŒ Broker 1 dies
          â–¼
T=0-6s    Detection period (heartbeat timeout)
          â³ Waiting...
          â³ Partition UNAVAILABLE
          â³ Producers/consumers retrying
          â–¼
T=6s      Controller detects failure
          ğŸ” Heartbeat timeout
          â–¼
T=6-6.1s  Leader election
          âš™ï¸  Select new leader
          âš™ï¸  Update metadata
          âš™ï¸  Notify brokers
          â–¼
T=6.1s+   Partition back online
          âœ“ Broker 2 is new leader
          âœ“ Producers/consumers reconnect
          âœ“ Normal operation resumed

EFFECTIVE DOWNTIME: ~100ms (election)
TOTAL UNAVAILABLE: ~6.1 seconds (including detection)
```

---

## Impact of Different Configurations

### Configuration 1: acks=0 (Fire and Forget)

```
Producer Config:
  acks = 0

Behavior:
  Producer â”€â”€â”€â”€ Message â”€â”€â”€â”€â–¶ Leader
  Producer doesn't wait for any acknowledgment
  Returns success immediately

SCENARIO: Leader fails right after receiving message

Timeline:
  T=0:   Producer sends message
  T=1ms: Message received by leader's network buffer
  T=1ms: Producer considers it "sent" (returns success)
  T=2ms: âŒ Leader crashes before writing to log

Result:
  âœ— Message LOST (never made it to log)
  âœ“ Producer thinks it was successful
  âœ— No retry (producer already moved on)

Risk Level: ğŸ”´ HIGHEST
Data Loss: Messages in network buffers or not yet written to disk

When to use: Metrics, logs, non-critical data where loss is acceptable
```

### Configuration 2: acks=1 (Leader Acknowledgment)

```
Producer Config:
  acks = 1
  retries = 3

Behavior:
  Producer â”€â”€â”€â”€ Message â”€â”€â”€â”€â–¶ Leader
                              Leader writes to log
  Producer â—„â”€â”€â”€ Ack â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  (Leader acknowledges BEFORE replication)

SCENARIO: Leader acknowledges, then crashes before replication

Timeline:
  T=0:    Producer sends message (offset 1000)
  T=10ms: Leader writes to local log
  T=11ms: Leader sends acknowledgment to producer
  T=12ms: Producer receives ack (considers successful)
  T=15ms: âŒ Leader crashes before followers fetch
  T=6s:   New leader elected (Broker 2)
  T=6s:   Broker 2 has messages 0-999 only

Result:
  âœ— Message 1000 LOST (was only on leader)
  âœ“ Producer thinks it was successful
  âœ— No retry (already acked)
  âœ“ Consumer never saw it (was above HWM)

Visual:
  Before crash:
    Leader:    [0...999][1000] â† Message 1000 acknowledged
    Follower:  [0...999]       â† Hasn't replicated yet

  After election:
    New Leader: [0...999]      â† Message 1000 gone forever

Risk Level: ğŸŸ¡ MEDIUM
Data Loss: Messages acknowledged but not yet replicated

When to use: Balanced approach, most common in production
Note: Acceptable if you can tolerate losing messages between
      acknowledgment and replication (typically milliseconds)
```

### Configuration 3: acks=all with min.insync.replicas=1 (Weak)

```
Producer Config:
  acks = all

Broker Config:
  min.insync.replicas = 1  â† Only leader needs to ack
  replication.factor = 3

Behavior:
  Same as acks=1 when all replicas are in ISR

Problem:
  With min.insync.replicas=1, "all" means "just the leader"
  Doesn't provide any additional safety over acks=1

Timeline:
  T=0:  Producer sends message
  T=5ms: Leader writes to log (LEO=1001)
  T=6ms: Leader sends ack (min.insync.replicas=1 satisfied)
  T=7ms: âŒ Leader crashes
  T=6s: New leader elected, message lost

Result:
  âœ— Same data loss as acks=1
  âœ— Slower (acks=all has overhead)
  âœ— False sense of security

Risk Level: ğŸŸ¡ MEDIUM (same as acks=1)
Data Loss: Yes, despite acks=all

When to use: âŒ DON'T USE THIS COMBINATION
Recommendation: If using acks=all, set min.insync.replicas >= 2
```

### Configuration 4: acks=all with min.insync.replicas=2 (Strong)

```
Producer Config:
  acks = all
  retries = 3

Broker Config:
  min.insync.replicas = 2
  replication.factor = 3

Behavior:
  Producer â”€â”€â”€â”€ Message â”€â”€â”€â”€â–¶ Leader
                              Leader writes to log
                              Leader replicates to followers
                              Wait for 1 follower ack (2 total including leader)
  Producer â—„â”€â”€â”€ Ack â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

SCENARIO: Leader crashes after replication

Timeline:
  T=0:    Producer sends message (offset 1000)
  T=5ms:  Leader writes to local log (LEO=1001)
  T=10ms: Leader sends to followers
  T=15ms: Broker 2 writes to log (LEO=1001)
  T=15ms: Broker 2 sends ack to leader
  T=16ms: Leader sends ack to producer (min.insync.replicas=2 satisfied)
  T=17ms: Producer receives ack
  T=20ms: Broker 3 writes to log (LEO=1001) [slower]
  T=25ms: âŒ Leader crashes
  T=6s:   New leader elected (Broker 2)
  T=6s:   Broker 2 has message 1000 âœ“

Result:
  âœ“ Message 1000 SAFE (on at least 2 brokers)
  âœ“ Producer knows it was successful
  âœ“ Consumer can eventually read it
  âœ“ No data loss

Visual:
  Before crash:
    Leader:     [0...999][1000] â† Acknowledged
    Follower 1: [0...999][1000] â† Replicated âœ“
    Follower 2: [0...999][1000] â† Replicated âœ“

  After election:
    New Leader: [0...999][1000] â† Message preserved!

Risk Level: ğŸŸ¢ LOW
Data Loss: Minimal (only if multiple brokers fail simultaneously)

When to use: âœ… RECOMMENDED for critical data
Trade-off: Slightly higher latency (~10-50ms more)
```

### Configuration 5: acks=all with min.insync.replicas=2 AND leader fails mid-replication

```
SCENARIO: Leader crashes AFTER 1 follower acks but BEFORE producer ack

Timeline:
  T=0:    Producer sends message (offset 1000)
  T=5ms:  Leader writes to log (LEO=1001)
  T=10ms: Leader replicates to followers
  T=15ms: Broker 2 writes and acks âœ“
  T=16ms: âŒ Leader crashes (BEFORE sending ack to producer)
  T=17ms: Producer timeout (doesn't receive ack)
  T=6s:   New leader elected (Broker 2)
  T=6s:   Producer retries
  T=6.1s: Message already exists (idempotent producer deduplicates)

Result:
  âœ“ Message 1000 exists on Broker 2
  âš ï¸  Producer doesn't know if it succeeded
  âœ“ Producer retries (idempotent producer prevents duplicates)
  âœ“ No data loss
  âœ“ No duplicates (with enable.idempotence=true)

Risk Level: ğŸŸ¢ LOW
Behavior: At-least-once delivery (or exactly-once with idempotence)
```

### Configuration 6: replication.factor=1 (No Replication)

```
Broker Config:
  replication.factor = 1  â† Only one copy exists

State:
  Only Broker 1 has the partition
  No followers
  No redundancy

Timeline:
  T=0:  Broker 1 has messages 0-1004
  T=1:  âŒ Broker 1 crashes
  T=6s: Controller detects failure
  T=6s: No other replicas exist
  T=6s: âš ï¸  PARTITION OFFLINE

Result:
  âœ— ALL messages 0-1004 UNAVAILABLE
  âœ— Partition cannot come online
  âœ— Must wait for Broker 1 to recover
  âœ— If disk corrupted: ALL DATA LOST

Risk Level: ğŸ”´ CATASTROPHIC
Data Loss: Everything if broker doesn't recover

When to use: âŒ NEVER in production
            âœ“ Only for development/testing
```

### Configuration 7: unclean.leader.election.enable=true

```
Broker Config:
  replication.factor = 3
  min.insync.replicas = 2
  unclean.leader.election.enable = true  â† Allow non-ISR as leader

SCENARIO: ALL ISR members are down, only non-ISR replica available

Setup:
  Broker 1 (Leader):    [0...1000] (DEAD)
  Broker 2 (ISR):       [0...1000] (DEAD)
  Broker 3 (non-ISR):   [0...850]  (ALIVE, lagging)

Without unclean election (default):
  - Partition stays OFFLINE
  - Wait for Broker 1 or 2 to recover
  - No data loss, but unavailable
  - Prioritizes consistency

With unclean election (enabled):
  - Broker 3 elected as leader (even though lagging)
  - Partition comes online
  - Messages 851-1000 LOST FOREVER
  - Prioritizes availability

Timeline:
  T=0:   Brokers 1 and 2 crash
  T=6s:  Controller detects failures
  T=6s:  ISR = [3] (only Broker 3 left)
  T=6s:  Broker 3 promoted to leader (unclean election)
  T=6s:  Partition ONLINE with messages 0-850
  T=7s:  Producer writes offset 851 (reusing lost offsets!)

  âš ï¸  Messages 851-1000 from old leader are GONE
  âš ï¸  New messages will use same offsets
  âš ï¸  Possible data inconsistency if consumers cached old data

Risk Level: ğŸ”´ HIGH
Data Loss: All unreplicated messages

When to use:
  âœ“ Systems where availability > consistency
  âœ“ Non-critical data (analytics, logs)
  âœ— Financial transactions
  âœ— Critical business data
```

---

## The Critical Question: Unreplicated Messages

### What Happens to Messages on Leader but Not Followers?

**Short Answer:** They are LOST forever.

**Why:**

```
FUNDAMENTAL PRINCIPLE:

Only messages below HWM are "committed"
HWM = lowest LEO among ISR members
If followers don't have it, it's not committed
Uncommitted messages can be lost

This is BY DESIGN for consistency
```

### Detailed Breakdown

```
BEFORE CRASH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Leader:    [0...999][1000,1001,1002,1003,1004]               â”‚
â”‚              â†‘                â†‘                                â”‚
â”‚              â”‚                â”‚                                â”‚
â”‚              â”‚                â””â”€ LEO = 1005                    â”‚
â”‚              â””â”€ HWM = 1000 (followers have up to 999)          â”‚
â”‚                                                                â”‚
â”‚  Follower 1: [0...999]                                         â”‚
â”‚               LEO = 1000                                       â”‚
â”‚                                                                â”‚
â”‚  Follower 2: [0...999]                                         â”‚
â”‚               LEO = 1000                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Status of messages 1000-1004:
  âœ— Not replicated
  âœ— Not visible to consumers (above HWM)
  âš ï¸  In limbo - acknowledged (if acks=1) but not committed

AFTER CRASH:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Leader: âŒ GONE                                               â”‚
â”‚                                                                â”‚
â”‚  New Leader (former Follower 1): [0...999]                     â”‚
â”‚                                   LEO = 1000                   â”‚
â”‚                                   HWM = 1000                   â”‚
â”‚                                                                â”‚
â”‚  Follower (former Follower 2): [0...999]                       â”‚
â”‚                                 LEO = 1000                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Messages 1000-1004:
  âœ— Physically gone (only existed on dead broker's disk)
  âœ— No way to recover them
  âœ— New leader starts accepting messages at offset 1000
  âš ï¸  Offset 1000 will be REUSED for new message!
```

### Why Can't We Recover Them?

```
Option 1: Wait for old leader to come back?
  âŒ Can't wait indefinitely (availability requirement)
  âŒ Old leader's disk might be corrupted
  âŒ Kafka prioritizes availability over waiting

Option 2: Use those messages from old leader when it returns?
  âŒ No! Old leader realizes it was partitioned
  âŒ Sees new leader epoch (6 vs its old epoch 5)
  âŒ Truncates its log back to HWM
  âŒ Messages 1000-1004 deleted to maintain consistency

Option 3: Keep them as "alternate timeline"?
  âŒ Would break consistency guarantees
  âŒ Consumers would see different data depending on timing
  âŒ Exactly-once semantics would be impossible
```

### What Happens When Old Leader Returns?

```
SCENARIO: Broker 1 comes back online

Current State:
  Broker 2 (Leader): [0...999][1000,1001] â† New messages
  Broker 3 (Follower): [0...999][1000,1001]
  Broker 1 (was leader, just recovered): [0...999][1000,1001,1002,1003,1004]
                                          â†‘ Old messages        â†‘ Old lost msgs

Recovery Process:

Step 1: Broker 1 rejoins cluster
  - Sends FetchRequest to new leader (Broker 2)
  - Includes its LEO: 1005
  - Includes old leader epoch: 5

Step 2: New leader responds with epoch information
  - Current leader epoch: 6
  - Divergence point: offset 1000
  - "You need to truncate back to 1000"

Step 3: Broker 1 truncates its log
  BEFORE: [0...999][1000,1001,1002,1003,1004]
  AFTER:  [0...999]

  âš ï¸  Messages 1000-1004 DELETED by Broker 1 itself!

Step 4: Broker 1 fetches from new leader
  Fetches offsets 1000-1001 (new messages)
  Now: [0...999][1000,1001]

Step 5: Broker 1 joins ISR
  ISR: [2, 3, 1]
  Broker 1 is now a follower

RESULT:
  Old "lost" messages 1000-1004 are permanently deleted
  New messages 1000-1001 are the canonical truth
  All replicas have consistent data
```

### Visual: Message Lifecycle

```
STAGE 1: Message Written to Leader
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Producer â†’ Leader â†’ Disk                â”‚
â”‚ Status: UNCOMMITTED                     â”‚
â”‚ Visible: NO                             â”‚
â”‚ Replicated: NO                          â”‚
â”‚ Safe: NO âŒ                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 2: Replication In Progress
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader â†’ Followers (network)            â”‚
â”‚ Status: UNCOMMITTED                     â”‚
â”‚ Visible: NO                             â”‚
â”‚ Replicated: PARTIAL                     â”‚
â”‚ Safe: NO âŒ                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 3: All ISR Members Have Message
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Leader + Followers (all ISR)            â”‚
â”‚ Status: UNCOMMITTED (still above HWM)   â”‚
â”‚ Visible: NO                             â”‚
â”‚ Replicated: YES                         â”‚
â”‚ Safe: YES âœ“                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 4: HWM Advanced
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ All ISR members acked                   â”‚
â”‚ Status: COMMITTED (below HWM)           â”‚
â”‚ Visible: YES                            â”‚
â”‚ Replicated: YES                         â”‚
â”‚ Safe: YES âœ“                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CRITICAL POINT:
  Leader can fail in Stages 1, 2, or 3 â†’ Message LOST
  Leader fails in Stage 4 â†’ Message SAFE
```

---

## Producer Behavior During Failure

### Producer Request Lifecycle

```
NORMAL OPERATION:

Producer                        Leader (Broker 1)
   â”‚                                 â”‚
   â”œâ”€â”€â”€â”€ ProduceRequest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚     (messages, acks=all)        â”‚
   â”‚                                 â”‚
   â”‚                                 â”œâ”€ Write to log
   â”‚                                 â”œâ”€ Replicate
   â”‚                                 â”œâ”€ Wait for ISR acks
   â”‚                                 â”‚
   â”‚â—€â”€â”€â”€â”€ ProduceResponse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚     (success, offset=1000)      â”‚
   â”‚                                 â”‚
   â””â”€ Continue with next batch       â”‚


DURING LEADER FAILURE:

Producer                        Leader (Broker 1)
   â”‚                                 â”‚
   â”œâ”€â”€â”€â”€ ProduceRequest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚                                 âœ— CRASH
   â”‚
   â”‚ (waiting for response...)
   â”‚
   â”‚ [request.timeout.ms expires]
   â”‚ (default: 30 seconds)
   â”‚
   â”‚ âŒ TimeoutException
   â”‚
   â”‚â”€ Check retries remaining
   â”‚  (default: retries = 2147483647)
   â”‚
   â”‚â”€ Refresh metadata
   â”‚  Who is the leader now?
   â”‚
   â”‚ (Wait for election... ~100ms)
   â”‚
   â”‚â”€ Metadata refreshed
   â”‚  New leader: Broker 2
   â”‚
   â”œâ”€â”€â”€â”€ ProduceRequest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Leader (Broker 2)
   â”‚                                  â”‚
   â”‚â—€â”€â”€â”€â”€ ProduceResponse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚     (success, offset=1000)       â”‚
   â”‚                                  â”‚
   â””â”€ Continue with next batch        â”‚
```

### Producer Configuration Impact

**Configuration Set 1: No Retries (Dangerous)**

```
Config:
  retries = 0
  acks = 1

Timeline:
  T=0:   Producer sends batch 1
  T=1:   Leader writes to log
  T=2:   Leader sends ack
  T=3:   Producer receives ack
  T=4:   Producer sends batch 2
  T=5:   âŒ Leader crashes
  T=5:   Producer gets NetworkException
  T=5:   retries=0 â†’ Give up immediately

Result:
  âœ— Batch 2 LOST
  âœ— Producer moves on
  âœ— Gap in data stream

When to use: âŒ Almost never
```

**Configuration Set 2: Retries Enabled (Standard)**

```
Config:
  retries = 2147483647 (infinite, default)
  retry.backoff.ms = 100
  request.timeout.ms = 30000
  delivery.timeout.ms = 120000 (2 minutes)
  max.in.flight.requests.per.connection = 5

Timeline:
  T=0:    Producer sends batch 1
  T=10ms: Batch 1 acked âœ“
  T=20ms: Producer sends batch 2
  T=30ms: âŒ Leader crashes
  T=30ms: Producer waits for response...
  T=30.1s: Timeout (request.timeout.ms)
  T=30.1s: Retry 1 - refresh metadata
  T=30.2s: Send to new leader (Broker 2)
  T=30.3s: Batch 2 acked âœ“

Result:
  âœ“ Batch 2 succeeds (after retry)
  âœ“ No data loss
  âš ï¸  30 second delay for that batch

When to use: âœ“ Standard production config
```

**Configuration Set 3: Idempotent Producer (Best)**

```
Config:
  enable.idempotence = true
  (Automatically sets: acks=all, retries=MAX, max.in.flight=5)

Behavior:
  Producer gets unique Producer ID (PID)
  Each message gets sequence number
  Broker tracks: PID + Sequence â†’ Offset mapping

Timeline:
  T=0:    Producer (PID=123) sends message (seq=0)
  T=10ms: Leader writes (offset=1000)
  T=11ms: âŒ Leader crashes before ack
  T=30s:  Request timeout
  T=30s:  Producer retries same message (PID=123, seq=0)
  T=30.1s: New leader checks: "I already have PID=123, seq=0"
  T=30.1s: New leader returns success with offset=1000 (idempotent)

Result:
  âœ“ No duplicates
  âœ“ Exactly-once to partition
  âœ“ Producer doesn't know if first send succeeded, doesn't matter

When to use: âœ“ Always enable (default in Kafka 3.0+)
```

### Producer Buffering

```
Producer Internal Buffer:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RECORD ACCUMULATOR                          â”‚
â”‚  (buffer.memory = 32 MB default)                               â”‚
â”‚                                                                â”‚
â”‚  Partition 0: [Batch1: 10 msgs][Batch2: 15 msgs][Batch3: ...]â”‚
â”‚  Partition 1: [Batch1: 8 msgs][Batch2: 12 msgs]              â”‚
â”‚  Partition 2: [Batch1: 20 msgs]                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                       â–²
             â”‚                       â”‚
             â”‚                       â””â”€ New sends go here
             â”‚
             â””â”€ Sender thread pulls batches from here

DURING LEADER FAILURE:

1. Sender thread can't send (leader down)
2. Application keeps calling producer.send()
3. Messages accumulate in buffer
4. Buffer fills up (32 MB)
5. Next producer.send() blocks for max.block.ms (default 60s)
6. If buffer still full after 60s â†’ TimeoutException

Timeline:
  T=0:     Leader fails
  T=0-30s: Requests timing out, retrying
  T=0-30s: New sends buffering in memory
  T=30s:   Buffer full (32 MB)
  T=30s:   producer.send() blocks
  T=30.1s: New leader elected
  T=30.2s: Sender drains buffer to new leader
  T=31s:   Buffer has space, producer.send() unblocks

Risk: If election takes > 60s, producer.send() throws exception
```

### Producer Guarantees Summary

| Configuration | Data Loss Risk | Duplicates Risk | Latency | Use Case |
|--------------|----------------|-----------------|---------|----------|
| acks=0 | High | None | Lowest | Metrics, logs |
| acks=1 | Medium | Possible | Low | Balanced |
| acks=all, min.isr=1 | Medium | Possible | Medium | âŒ Don't use |
| acks=all, min.isr=2 | Low | Possible | Higher | Critical data |
| acks=all, min.isr=2, idempotent | Lowest | None | Higher | âœ… Best practice |

---

## Consumer Behavior During Failure

### Consumer Fetch Process

```
NORMAL OPERATION:

Consumer                        Leader (Broker 1)
   â”‚                                 â”‚
   â”œâ”€â”€â”€â”€ FetchRequest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚     (offset=1000)               â”‚
   â”‚                                 â”‚
   â”‚                                 â”œâ”€ Check offset exists
   â”‚                                 â”œâ”€ Offset below HWM? âœ“
   â”‚                                 â”œâ”€ Read from disk/cache
   â”‚                                 â”‚
   â”‚â—€â”€â”€â”€â”€ FetchResponse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚     (messages 1000-1099)        â”‚
   â”‚                                 â”‚
   â”‚â”€ Process messages               â”‚
   â”‚â”€ Commit offset 1100             â”‚
   â”‚                                 â”‚


DURING LEADER FAILURE:

Consumer                        Leader (Broker 1)      New Leader (Broker 2)
   â”‚                                 â”‚                      â”‚
   â”œâ”€â”€â”€â”€ FetchRequest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚                      â”‚
   â”‚     (offset=1000)               âœ— CRASH               â”‚
   â”‚                                                        â”‚
   â”‚ (waiting for response...)                             â”‚
   â”‚                                                        â”‚
   â”‚ [request.timeout.ms expires]                          â”‚
   â”‚                                                        â”‚
   â”‚ âŒ NetworkException                                   â”‚
   â”‚                                                        â”‚
   â”‚â”€ Refresh metadata                                     â”‚
   â”‚  (discovers new leader)                               â”‚
   â”‚                                                        â”‚
   â”œâ”€â”€â”€â”€ FetchRequest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
   â”‚     (offset=1000)                                      â”‚
   â”‚                                                        â”‚
   â”‚â—€â”€â”€â”€â”€ FetchResponse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚     (messages 1000-1099)                               â”‚
   â”‚                                                        â”‚
   â”‚â”€ Process messages                                      â”‚
   â”‚â”€ Commit offset 1100                                    â”‚
```

### Key Consumer Observations

**1. Consumers Never See Uncommitted Messages**

```
Scenario:
  Leader has messages 0-1004
  Followers have messages 0-999
  HWM = 1000
  Leader crashes

Consumer behavior:
  âœ“ Consumer last fetched up to offset 999
  âœ“ Consumer committed offset 1000 (next to read)
  âŒ Leader fails
  âœ“ New leader has up to offset 999
  âœ“ Consumer resumes from offset 1000...
  âš ï¸  But new leader's next message is ALSO offset 1000 (new msg)

Result:
  âœ“ Consumer has no gap in offsets
  âœ“ Consumer never knew messages 1000-1004 existed on old leader
  âœ“ Seamless from consumer perspective
```

**2. Consumer Committed Offsets Are Safe**

```
Consumer offset storage (in __consumer_offsets topic):
  - Also replicated (typically RF=3)
  - Also has min.insync.replicas
  - Survives broker failures

Scenario:
  Consumer commits offset 500
  âŒ Consumer crashes
  âŒ Broker 1 (partition leader) crashes
  âœ“ New consumer instance starts
  âœ“ Reads committed offset: 500
  âœ“ Resumes from offset 500
```

**3. Consumer Rebalancing During Broker Failure**

```
IF consumer was fetching from failed broker:
  âœ“ Consumer refreshes metadata
  âœ“ Connects to new leader
  âœ“ Continues from last committed offset
  âœ“ No rebalance needed (partition assignment unchanged)

IF consumer is part of consumer group:
  âœ“ Group coordinator tracks consumer health
  âœ“ Consumer still sends heartbeats (to coordinator, not data broker)
  âœ“ Fetching failures don't trigger rebalance
  âœ“ Consumer auto-retries fetching
```

### Consumer Configuration Impact

**Auto-commit Enabled (Default)**

```
Config:
  enable.auto.commit = true
  auto.commit.interval.ms = 5000 (5 seconds)

Behavior:
  T=0s:   Fetch messages 0-99
  T=0.1s: Process messages 0-99
  T=5s:   Auto-commit offset 100
  T=5.1s: Fetch messages 100-199
  T=5.2s: âŒ Consumer crashes

  T=10s:  New consumer starts
  T=10s:  Reads committed offset: 100
  T=10s:  Resumes from offset 100

Result:
  âœ“ No message loss
  âœ— Messages 100-199 might be reprocessed (not committed)
  âš ï¸  At-least-once delivery

Risk: Duplicate processing if consumer crashes between fetch and commit
```

**Manual Commit (Safer)**

```
Config:
  enable.auto.commit = false

Code:
  while (true) {
    records = consumer.poll(Duration.ofMillis(100));

    for (record : records) {
      process(record);
    }

    consumer.commitSync();  // Commit after processing
  }

Behavior:
  T=0s:   Fetch messages 0-99
  T=0.1s: Process messages 0-99
  T=0.2s: Commit offset 100 âœ“
  T=0.3s: Fetch messages 100-199
  T=0.4s: Process messages 100-150
  T=0.5s: âŒ Consumer crashes (before commit)

  T=10s:  New consumer starts
  T=10s:  Reads committed offset: 100
  T=10s:  Resumes from offset 100

Result:
  âœ“ No message loss
  âš ï¸  Messages 100-150 reprocessed (were processed but not committed)
  âœ“ Better control than auto-commit

When to use: âœ“ When processing must be complete before commit
```

**Read Committed (For Transactional Producers)**

```
Config:
  isolation.level = read_committed

Behavior:
  Consumer only sees messages from committed transactions
  Skips aborted transaction messages
  Waits at LSO (Last Stable Offset) instead of HWM

Scenario:
  Offset 100: Message A (committed transaction)
  Offset 101: Message B (transaction in progress)
  Offset 102: Message C (transaction in progress)
  Offset 103: Message D (committed transaction)

  Consumer with read_uncommitted:
    Sees: A, B, C, D (all messages)

  Consumer with read_committed:
    Sees: A (stops here until transaction commits)
    Later (after commit): A, B, C, D

When to use: âœ“ With exactly-once semantics (EOS)
```

---

## Data Loss Scenarios

### Scenario 1: acks=1, Leader Fails Immediately

```
Config:
  acks = 1
  replication.factor = 3

Timeline:
  0ms:   Producer sends message M1
  10ms:  Leader writes M1 to disk (offset 1000)
  11ms:  Leader sends ack to producer âœ“
  12ms:  Producer receives ack (considers successful)
  13ms:  âŒ Leader fails (before followers replicate)
  6s:    New leader elected
  6s:    New leader doesn't have M1

Result:
  âœ— Message M1 LOST
  âœ“ Producer thinks it succeeded
  âœ— Cannot retry (already acked)
  âœ“ Consumer never saw it (was above HWM)

Probability: Low (milliseconds window)
Data Loss: 1 message
Impact: Silent data loss

Mitigation: Use acks=all
```

### Scenario 2: acks=all, Both Leader and Follower Fail

```
Config:
  acks = all
  min.insync.replicas = 2
  replication.factor = 3

Timeline:
  0ms:   Producer sends message M1
  10ms:  Leader (B1) writes M1
  15ms:  Follower 1 (B2) writes M1
  16ms:  Leader sends ack to producer âœ“
  17ms:  Producer receives ack
  20ms:  Follower 2 (B3) is slow, hasn't written yet
  25ms:  âŒ Leader (B1) fails
  26ms:  âŒ Follower 1 (B2) also fails (correlated failure!)
  6s:    Only Follower 2 (B3) available
  6s:    Follower 2 doesn't have M1
  6s:

  Option A (unclean.leader.election.enable = false):
    6s: Partition goes OFFLINE
    6s: Wait for B1 or B2 to recover
    Result: No data loss, but unavailable

  Option B (unclean.leader.election.enable = true):
    6s: B3 elected as leader (unclean election)
    6s: M1 lost
    Result: Available but data loss

Probability: Very low (correlated failures rare)
Data Loss: Messages not replicated to surviving broker
Impact: Depends on unclean election setting

Mitigation:
  - Rack awareness
  - Higher replication factor (5 instead of 3)
  - Better infrastructure (avoid correlated failures)
```

### Scenario 3: Replication Factor = 1

```
Config:
  replication.factor = 1
  (Only one copy exists)

Timeline:
  0ms:  Broker 1 has messages 0-1000
  1ms:  âŒ Broker 1 fails
  6s:   No other replicas
  6s:   Partition OFFLINE

  Option A: Broker comes back, disk OK
    â†’ All data recovered âœ“

  Option B: Broker comes back, disk corrupted
    â†’ ALL DATA LOST âœ—

  Option C: Broker never comes back
    â†’ ALL DATA LOST âœ—

Probability: 100% (if broker doesn't recover)
Data Loss: EVERYTHING
Impact: Catastrophic

Mitigation: âŒ NEVER use RF=1 in production
```

### Scenario 4: Committed Offsets Lost

```
Config (for __consumer_offsets topic):
  replication.factor = 1  â† Misconfigured!

Setup:
  Consumer Group: order-processors
  Committed offsets stored in __consumer_offsets-0 on Broker 4
  Consumer has processed messages 0-500

Timeline:
  0ms:  Consumer commits offset 501
  1ms:  âŒ Broker 4 fails (and doesn't recover)
  6s:   __consumer_offsets-0 LOST
  7s:   New consumer joins group
  7s:   No committed offset found
  7s:   Defaults to auto.offset.reset = latest
  8s:   Consumer starts from end (offset 1000)

Result:
  âœ— Messages 501-999 SKIPPED
  âœ— Data loss from consumer perspective
  âœ“ Messages still in Kafka
  âœ— But consumer doesn't know where it was

Mitigation:
  âœ“ Ensure offsets.topic.replication.factor = 3 (default)
  âœ“ Set auto.offset.reset = earliest (if tolerate reprocessing)
  âœ“ External offset store (database) as backup
```

### Scenario 5: Split Brain (Very Rare)

```
Situation:
  Network partition isolates old leader from cluster
  Old leader doesn't know it's been replaced

Timeline:
  0s:    Broker 1 is leader
  0.5s:  Network partition: B1 isolated from B2, B3, Controller
  1s:    B1 still thinks it's leader (hasn't heard otherwise)
  6s:    Controller (can't reach B1) elects B2 as new leader
  7s:    Producer P1 (on B1's network side) writes to B1 (old leader)
  7s:    Producer P2 (on B2's network side) writes to B2 (new leader)
  8s:    Network partition heals
  8s:    B1 discovers new leader epoch
  8s:    B1 truncates divergent messages

Result:
  âœ— Messages written to B1 during partition are LOST
  âœ“ Messages written to B2 (real leader) are safe
  âš ï¸  "Split brain" data divergence

Mitigation:
  âœ“ Leader epoch mechanism (Kafka has this)
  âœ“ Fencing: B1 stops accepting writes when sees higher epoch
  âœ“ Idempotent producers (detect and reject duplicates)

Kafka's Protection:
  Modern Kafka (0.11+) has strong protections:
  - Leader epochs prevent split brain
  - Old leader rejects requests when it sees higher epoch
  - Very rare in practice
```

---

## Configuration Best Practices

### For Critical Data (Financial, Orders, User Data)

```yaml
# PRODUCER CONFIG
acks: all
retries: 2147483647  # Infinite retries
retry.backoff.ms: 100
request.timeout.ms: 30000
delivery.timeout.ms: 120000  # 2 minutes total
enable.idempotence: true
max.in.flight.requests.per.connection: 5
compression.type: lz4  # Good balance of speed/ratio

# TOPIC CONFIG
replication.factor: 3  # Minimum
min.insync.replicas: 2  # Must have 2 replicas
unclean.leader.election.enable: false  # No data loss

# CONSUMER CONFIG
enable.auto.commit: false  # Manual commit after processing
isolation.level: read_committed  # If using transactions
auto.offset.reset: earliest  # Reprocess rather than skip

# CLUSTER CONFIG
offsets.topic.replication.factor: 3
transaction.state.log.replication.factor: 3
```

**Guarantees:**
- âœ“ No data loss (even with single broker failure)
- âœ“ Exactly-once semantics (with transactions)
- âœ“ Can tolerate 1 broker failure
- âš ï¸  Slightly higher latency (~10-50ms)

### For High-Throughput Non-Critical Data (Logs, Metrics)

```yaml
# PRODUCER CONFIG
acks: 1  # Leader only
retries: 5  # Limited retries
request.timeout.ms: 30000
enable.idempotence: false
compression.type: snappy  # Fast compression
batch.size: 32768  # Larger batches
linger.ms: 10  # Small batching delay

# TOPIC CONFIG
replication.factor: 2  # Reduced redundancy
min.insync.replicas: 1  # Leader only
unclean.leader.election.enable: true  # Availability over consistency

# CONSUMER CONFIG
enable.auto.commit: true
auto.commit.interval.ms: 5000
auto.offset.reset: latest  # Skip old data if needed

# CLUSTER CONFIG
offsets.topic.replication.factor: 3  # Keep offset safety
```

**Guarantees:**
- âš ï¸  Some data loss possible
- âœ“ Higher throughput
- âœ“ Lower latency
- âœ“ Better availability

### For Balanced Production Workloads

```yaml
# PRODUCER CONFIG
acks: all
retries: 2147483647
request.timeout.ms: 30000
delivery.timeout.ms: 120000
enable.idempotence: true
compression.type: lz4
batch.size: 16384
linger.ms: 0

# TOPIC CONFIG
replication.factor: 3
min.insync.replicas: 2
unclean.leader.election.enable: false

# CONSUMER CONFIG
enable.auto.commit: false
isolation.level: read_committed
auto.offset.reset: earliest

# CLUSTER CONFIG
offsets.topic.replication.factor: 3
transaction.state.log.replication.factor: 3
replica.lag.time.max.ms: 10000
```

**Guarantees:**
- âœ“ No data loss with single failure
- âœ“ Good throughput
- âœ“ Reasonable latency
- âœ“ Exactly-once capable

---

## Summary: Quick Reference

### What Messages Are Lost When Leader Fails?

| Message State | Lost? | Why? |
|---------------|-------|------|
| Below HWM (committed) | âŒ No | Replicated to all ISR members |
| Above HWM, replicated to some ISR | âŒ No | If those replicas become leader |
| Above HWM, only on leader | âœ… Yes | No replicas have it |
| Acknowledged with acks=1, not replicated | âœ… Yes | Producer thinks it's sent but it's lost |
| Acknowledged with acks=all, ISR have it | âŒ No | Safe on multiple brokers |
| In producer buffer, not sent | âŒ No | Producer retries after metadata refresh |

### Consumer Perspective

```
Question: Will consumer see data loss?

Answer: Consumers NEVER see data loss from leader failure

Why:
  1. Consumers only read below HWM
  2. HWM = what all ISR replicas have
  3. If leader fails, new leader has all HWM data
  4. Consumer committed offsets are also replicated
  5. Consumer resumes exactly where it left off

BUT consumers may see:
  âœ“ Temporary unavailability (during election, ~100ms)
  âœ“ Latency spike (during metadata refresh)
  âœ— Never missing messages
  âœ— Never out-of-order messages (within partition)
```

### Downtime During Leader Failure

```
Component          Downtime            Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Detection          0-6 seconds         Heartbeat timeout
Election           50-200ms            Usually ~100ms
Metadata Propagation 10-100ms          Clients refresh metadata
Producer Recovery  0-30s               Depends on request timeout
Consumer Recovery  0-30s               Depends on request timeout

Total Unavailability: ~6-7 seconds (including detection)
Effective Downtime: ~100-300ms (just the election)

Modern optimizations:
- KRaft: Faster metadata operations
- Incremental rebalancing: Less consumer impact
- Background metadata refresh: Faster client recovery
```

### Configuration Cheat Sheet

**Maximum Durability (No Data Loss):**
```
acks=all + min.insync.replicas=2 + RF=3
unclean.leader.election.enable=false
enable.idempotence=true
```

**Maximum Availability (Tolerate Data Loss):**
```
acks=1 + min.insync.replicas=1 + RF=2
unclean.leader.election.enable=true
enable.idempotence=false
```

**Balanced (Recommended):**
```
acks=all + min.insync.replicas=2 + RF=3
unclean.leader.election.enable=false
enable.idempotence=true
```

### Key Takeaways

1. **Messages above HWM are at risk** - If leader fails before replication completes, these messages are lost forever.

2. **HWM protects consumers** - Consumers never see unreplicated data, so they never experience "data loss" from their perspective.

3. **Configuration matters immensely** - acks=all with min.insync.replicas=2 prevents data loss. acks=1 can lose data.

4. **Detection time is the bottleneck** - 6 seconds to detect failure, only 100ms for election.

5. **Old leader truncates on return** - When old leader recovers, it deletes divergent messages to maintain consistency.

6. **Idempotent producers are essential** - Enable exactly-once semantics and prevent duplicates during retries.

7. **Unclean election is dangerous** - Only enable for non-critical data where availability > consistency.

8. **Replication Factor = 1 is production suicide** - Never use RF=1 for anything important.

---

**Remember:** Kafka is designed for high availability with configurable durability. The key is understanding the trade-offs and choosing the right configuration for your use case.
